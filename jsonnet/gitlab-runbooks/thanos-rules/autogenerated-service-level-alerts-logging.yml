# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./thanos-rules-jsonnet/service-component-alerts.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: 'Service Component Alerts: logging'
  interval: 1m
  partial_response_strategy: warn
  rules:
  - alert: LoggingServiceElasticsearchIndexingClusterTrafficCessation
    for: 5m
    annotations:
      title: The elasticsearch_indexing_cluster SLI of the logging service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        This cluster SLI monitors log index operations to GitLab's logging ELK instance.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1468912914"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          clamp_min(deriv(elasticsearch_indices_indexing_index_total{type="logging", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m]), 0)
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{component="elasticsearch_indexing_cluster",monitor="global",stage="main",type="logging"} == 0
      and
      gitlab_component_ops:rate_30m{component="elasticsearch_indexing_cluster",monitor="global",stage="main",type="logging"} offset 1h >= 0.16666666666666666
  - alert: LoggingServiceElasticsearchIndexingClusterTrafficAbsent
    for: 30m
    annotations:
      title: The elasticsearch_indexing_cluster SLI of the logging service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        This cluster SLI monitors log index operations to GitLab's logging ELK instance.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1468912914"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          clamp_min(deriv(elasticsearch_indices_indexing_index_total{type="logging", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m]), 0)
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{component="elasticsearch_indexing_cluster",monitor="global",stage="main",type="logging"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="elasticsearch_indexing_cluster",monitor="global",stage="main",type="logging"}
  - alert: LoggingServiceElasticsearchIndexingIndexTrafficCessation
    for: 5m
    annotations:
      title: The elasticsearch_indexing_index SLI of the logging service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        This index SLI monitors log index operations to GitLab's logging ELK instance.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2294808141"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          clamp_min(deriv(elasticsearch_index_stats_indexing_index_total{type="logging", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m]), 0)
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{component="elasticsearch_indexing_index",monitor="global",stage="main",type="logging"} == 0
      and
      gitlab_component_ops:rate_30m{component="elasticsearch_indexing_index",monitor="global",stage="main",type="logging"} offset 1h >= 0.16666666666666666
  - alert: LoggingServiceElasticsearchIndexingIndexTrafficAbsent
    for: 30m
    annotations:
      title: The elasticsearch_indexing_index SLI of the logging service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        This index SLI monitors log index operations to GitLab's logging ELK instance.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2294808141"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          clamp_min(deriv(elasticsearch_index_stats_indexing_index_total{type="logging", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m]), 0)
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{component="elasticsearch_indexing_index",monitor="global",stage="main",type="logging"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="elasticsearch_indexing_index",monitor="global",stage="main",type="logging"}
  - alert: LoggingServiceElasticsearchSearchingClusterTrafficCessation
    for: 5m
    annotations:
      title: The elasticsearch_searching_cluster SLI of the logging service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        This cluster SLI monitors searches issued to GitLab's logging ELK instance.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4214347698"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          clamp_min(deriv(elasticsearch_indices_search_query_total{type="logging", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m]), 0)
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{component="elasticsearch_searching_cluster",monitor="global",stage="main",type="logging"} == 0
      and
      gitlab_component_ops:rate_30m{component="elasticsearch_searching_cluster",monitor="global",stage="main",type="logging"} offset 1h >= 0.16666666666666666
  - alert: LoggingServiceElasticsearchSearchingClusterTrafficAbsent
    for: 30m
    annotations:
      title: The elasticsearch_searching_cluster SLI of the logging service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        This cluster SLI monitors searches issued to GitLab's logging ELK instance.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4214347698"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          clamp_min(deriv(elasticsearch_indices_search_query_total{type="logging", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m]), 0)
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{component="elasticsearch_searching_cluster",monitor="global",stage="main",type="logging"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="elasticsearch_searching_cluster",monitor="global",stage="main",type="logging"}
  - alert: LoggingServiceElasticsearchSearchingIndexTrafficCessation
    for: 5m
    annotations:
      title: The elasticsearch_searching_index SLI of the logging service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        This index SLI monitors searches issued to GitLab's logging ELK instance.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2852417009"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          clamp_min(deriv(elasticsearch_index_stats_search_query_total{type="logging", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m]), 0)
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{component="elasticsearch_searching_index",monitor="global",stage="main",type="logging"} == 0
      and
      gitlab_component_ops:rate_30m{component="elasticsearch_searching_index",monitor="global",stage="main",type="logging"} offset 1h >= 0.16666666666666666
  - alert: LoggingServiceElasticsearchSearchingIndexTrafficAbsent
    for: 30m
    annotations:
      title: The elasticsearch_searching_index SLI of the logging service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        This index SLI monitors searches issued to GitLab's logging ELK instance.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2852417009"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          clamp_min(deriv(elasticsearch_index_stats_search_query_total{type="logging", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m]), 0)
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{component="elasticsearch_searching_index",monitor="global",stage="main",type="logging"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="elasticsearch_searching_index",monitor="global",stage="main",type="logging"}
  - alert: LoggingServiceFluentdLogOutputErrorSLOViolation
    for: 2m
    annotations:
      title: The fluentd_log_output SLI of the logging service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors fluentd log output and the number of output errors in fluentd.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3405007546"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(fluentd_output_status_num_errors{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="fluentd_log_output",monitor="global",type="logging"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="fluentd_log_output",monitor="global",type="logging"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="fluentd_log_output",monitor="global",type="logging"}) >= 1
      )
  - alert: LoggingServiceFluentdLogOutputErrorSLOViolation
    for: 2m
    annotations:
      title: The fluentd_log_output SLI of the logging service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors fluentd log output and the number of output errors in fluentd.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3405007546"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(fluentd_output_status_num_errors{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="fluentd_log_output",monitor="global",type="logging"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="fluentd_log_output",monitor="global",type="logging"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="fluentd_log_output",monitor="global",type="logging"}) >= 0.16667
      )
  - alert: LoggingServiceFluentdLogOutputTrafficCessation
    for: 5m
    annotations:
      title: The fluentd_log_output SLI of the logging service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        This SLI monitors fluentd log output and the number of output errors in fluentd.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3827296992"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(fluentd_output_status_write_count{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{component="fluentd_log_output",monitor="global",stage="main",type="logging"} == 0
      and
      gitlab_component_ops:rate_30m{component="fluentd_log_output",monitor="global",stage="main",type="logging"} offset 1h >= 0.16666666666666666
  - alert: LoggingServiceFluentdLogOutputTrafficAbsent
    for: 30m
    annotations:
      title: The fluentd_log_output SLI of the logging service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        This SLI monitors fluentd log output and the number of output errors in fluentd.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3827296992"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(fluentd_output_status_write_count{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{component="fluentd_log_output",monitor="global",stage="main",type="logging"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="fluentd_log_output",monitor="global",stage="main",type="logging"}
  - alert: LoggingServiceKibanaGooglelbErrorSLOViolation
    for: 2m
    annotations:
      title: The kibana_googlelb SLI of the logging service (`{{ $labels.stage }}`
        stage) has an error rate violating SLO
      description: |
        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1918266605"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(stackdriver_https_lb_rule_loadbalancing_googleapis_com_https_request_count{environment="{{ $labels.environment }}",project_id="gitlab-ops",response_code_class="500",stage="{{ $labels.stage }}",url_map_name="ops-prod-proxy"}[5m])
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="kibana_googlelb",monitor="global",type="logging"}
          > (14.4 * 0.005000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="kibana_googlelb",monitor="global",type="logging"}
          > (14.4 * 0.005000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="kibana_googlelb",monitor="global",type="logging"}) >= 1
      )
  - alert: LoggingServiceKibanaGooglelbErrorSLOViolation
    for: 2m
    annotations:
      title: The kibana_googlelb SLI of the logging service (`{{ $labels.stage }}`
        stage) has an error rate violating SLO
      description: |
        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1918266605"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(stackdriver_https_lb_rule_loadbalancing_googleapis_com_https_request_count{environment="{{ $labels.environment }}",project_id="gitlab-ops",response_code_class="500",stage="{{ $labels.stage }}",url_map_name="ops-prod-proxy"}[5m])
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="kibana_googlelb",monitor="global",type="logging"}
          > (6 * 0.005000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="kibana_googlelb",monitor="global",type="logging"}
          > (6 * 0.005000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="kibana_googlelb",monitor="global",type="logging"}) >= 0.16667
      )
  - alert: LoggingServicePubsubSubscriptionsTrafficCessation
    for: 5m
    annotations:
      title: The pubsub_subscriptions SLI of the logging service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        This SLI monitors pubsub subscriptions.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3855936082"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(stackdriver_pubsub_subscription_pubsub_googleapis_com_subscription_byte_cost{type="monitoring", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{component="pubsub_subscriptions",monitor="global",stage="main",type="logging"} == 0
      and
      gitlab_component_ops:rate_30m{component="pubsub_subscriptions",monitor="global",stage="main",type="logging"} offset 1h >= 0.16666666666666666
  - alert: LoggingServicePubsubSubscriptionsTrafficAbsent
    for: 30m
    annotations:
      title: The pubsub_subscriptions SLI of the logging service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        This SLI monitors pubsub subscriptions.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3855936082"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(stackdriver_pubsub_subscription_pubsub_googleapis_com_subscription_byte_cost{type="monitoring", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{component="pubsub_subscriptions",monitor="global",stage="main",type="logging"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="pubsub_subscriptions",monitor="global",stage="main",type="logging"}
  - alert: LoggingServicePubsubTopicsTrafficCessation
    for: 5m
    annotations:
      title: The pubsub_topics SLI of the logging service (`{{ $labels.stage }}` stage)
        has not received any traffic in the past 30m
      description: |
        This SLI monitors pubsub topics.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "705066779"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(stackdriver_pubsub_topic_pubsub_googleapis_com_topic_byte_cost{type="monitoring", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{component="pubsub_topics",monitor="global",stage="main",type="logging"} == 0
      and
      gitlab_component_ops:rate_30m{component="pubsub_topics",monitor="global",stage="main",type="logging"} offset 1h >= 0.16666666666666666
  - alert: LoggingServicePubsubTopicsTrafficAbsent
    for: 30m
    annotations:
      title: The pubsub_topics SLI of the logging service (`{{ $labels.stage }}` stage)
        has not reported any traffic in the past 30m
      description: |
        This SLI monitors pubsub topics.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "705066779"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(stackdriver_pubsub_topic_pubsub_googleapis_com_topic_byte_cost{type="monitoring", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{component="pubsub_topics",monitor="global",stage="main",type="logging"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="pubsub_topics",monitor="global",stage="main",type="logging"}
  - alert: LoggingServicePubsubbeatErrorSLOViolation
    for: 2m
    annotations:
      title: The pubsubbeat SLI of the logging service (`{{ $labels.stage }}` stage)
        has an error rate violating SLO
      description: |
        This SLI monitors pubsubbeat errors.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2430725542"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(pubsubbeat_errors_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="pubsubbeat",monitor="global",type="logging"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="pubsubbeat",monitor="global",type="logging"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="pubsubbeat",monitor="global",type="logging"}) >= 1
      )
  - alert: LoggingServicePubsubbeatErrorSLOViolation
    for: 2m
    annotations:
      title: The pubsubbeat SLI of the logging service (`{{ $labels.stage }}` stage)
        has an error rate violating SLO
      description: |
        This SLI monitors pubsubbeat errors.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2430725542"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(pubsubbeat_errors_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="pubsubbeat",monitor="global",type="logging"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="pubsubbeat",monitor="global",type="logging"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="pubsubbeat",monitor="global",type="logging"}) >= 0.16667
      )
  - alert: LoggingServicePubsubbeatTrafficCessation
    for: 5m
    annotations:
      title: The pubsubbeat SLI of the logging service (`{{ $labels.stage }}` stage)
        has not received any traffic in the past 30m
      description: |
        This SLI monitors pubsubbeat errors.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1522766756"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(pubsubbeat_libbeat_output_events{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{component="pubsubbeat",monitor="global",stage="main",type="logging"} == 0
      and
      gitlab_component_ops:rate_30m{component="pubsubbeat",monitor="global",stage="main",type="logging"} offset 1h >= 0.16666666666666666
  - alert: LoggingServicePubsubbeatTrafficAbsent
    for: 30m
    annotations:
      title: The pubsubbeat SLI of the logging service (`{{ $labels.stage }}` stage)
        has not reported any traffic in the past 30m
      description: |
        This SLI monitors pubsubbeat errors.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: logging-main/logging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/logging-main/logging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1522766756"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(pubsubbeat_libbeat_output_events{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/logging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{component="pubsubbeat",monitor="global",stage="main",type="logging"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="pubsubbeat",monitor="global",stage="main",type="logging"}
