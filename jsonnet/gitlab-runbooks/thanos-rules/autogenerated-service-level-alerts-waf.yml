# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./thanos-rules-jsonnet/service-component-alerts.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: 'Service Component Alerts: waf'
  interval: 1m
  partial_response_strategy: warn
  rules:
  - alert: WafServiceGitlabNetZoneTrafficCessation
    for: 5m
    annotations:
      title: The gitlab_net_zone SLI of the waf service (`{{ $labels.stage }}` stage)
        has not received any traffic in the past 30m
      description: |
        Aggregation of all GitLab.net (non-pulic) traffic passing through the WAF.

        Errors on this SLI may indicate that the WAF has detected malicious traffic and is blocking it.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: waf-main/waf-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/waf-main/waf-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3342493164"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(cloudflare_zones_http_responses_total{zone="gitlab.net", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/waf/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{component="gitlab_net_zone",monitor="global",stage="main",type="waf"} == 0
      and
      gitlab_component_ops:rate_30m{component="gitlab_net_zone",monitor="global",stage="main",type="waf"} offset 1h >= 0.16666666666666666
  - alert: WafServiceGitlabNetZoneTrafficAbsent
    for: 30m
    annotations:
      title: The gitlab_net_zone SLI of the waf service (`{{ $labels.stage }}` stage)
        has not reported any traffic in the past 30m
      description: |
        Aggregation of all GitLab.net (non-pulic) traffic passing through the WAF.

        Errors on this SLI may indicate that the WAF has detected malicious traffic and is blocking it.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: waf-main/waf-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/waf-main/waf-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3342493164"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(cloudflare_zones_http_responses_total{zone="gitlab.net", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/waf/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{component="gitlab_net_zone",monitor="global",stage="main",type="waf"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="gitlab_net_zone",monitor="global",stage="main",type="waf"}
  - alert: WafServiceGitlabZoneTrafficCessation
    for: 5m
    annotations:
      title: The gitlab_zone SLI of the waf service (`{{ $labels.stage }}` stage)
        has not received any traffic in the past 30m
      description: |
        Aggregation of all public traffic for GitLab.com passing through the WAF.

        Errors on this SLI may indicate that the WAF has detected malicious traffic and is blocking it. It may also indicate serious upstream failures on GitLab.com.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: waf-main/waf-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/waf-main/waf-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "443878888"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(cloudflare_zones_http_responses_total{zone=~"gitlab.com|staging.gitlab.com", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/waf/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{component="gitlab_zone",monitor="global",stage="main",type="waf"} == 0
      and
      gitlab_component_ops:rate_30m{component="gitlab_zone",monitor="global",stage="main",type="waf"} offset 1h >= 0.16666666666666666
  - alert: WafServiceGitlabZoneTrafficAbsent
    for: 30m
    annotations:
      title: The gitlab_zone SLI of the waf service (`{{ $labels.stage }}` stage)
        has not reported any traffic in the past 30m
      description: |
        Aggregation of all public traffic for GitLab.com passing through the WAF.

        Errors on this SLI may indicate that the WAF has detected malicious traffic and is blocking it. It may also indicate serious upstream failures on GitLab.com.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: waf-main/waf-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/waf-main/waf-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "443878888"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(cloudflare_zones_http_responses_total{zone=~"gitlab.com|staging.gitlab.com", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/waf/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{component="gitlab_zone",monitor="global",stage="main",type="waf"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="gitlab_zone",monitor="global",stage="main",type="waf"}
