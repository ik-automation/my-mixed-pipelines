# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./thanos-rules-jsonnet/service-component-alerts.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: 'Service Component Alerts: redis-sidekiq'
  interval: 1m
  partial_response_strategy: warn
  rules:
  - alert: RedisSidekiqServicePrimaryServerTrafficCessation
    for: 5m
    annotations:
      title: The primary_server SLI of the redis-sidekiq service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        Operations on the Redis primary for Redis Sidekiq instance.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "175328272"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(redis_commands_processed_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="redis-sidekiq"}[5m]) and on (instance) redis_instance_info{role="master"}
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_30m{component="primary_server",monitor="global",stage="main",type="redis-sidekiq"} == 0
      and
      gitlab_component_ops:rate_30m{component="primary_server",monitor="global",stage="main",type="redis-sidekiq"} offset 1h >= 0.16666666666666666
  - alert: RedisSidekiqServicePrimaryServerTrafficAbsent
    for: 30m
    annotations:
      title: The primary_server SLI of the redis-sidekiq service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        Operations on the Redis primary for Redis Sidekiq instance.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "175328272"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(redis_commands_processed_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="redis-sidekiq"}[5m]) and on (instance) redis_instance_info{role="master"}
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_5m{component="primary_server",monitor="global",stage="main",type="redis-sidekiq"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="primary_server",monitor="global",stage="main",type="redis-sidekiq"}
  - alert: RedisSidekiqServiceRailsRedisClientApdexSLOViolation
    for: 2m
    annotations:
      title: The rails_redis_client SLI of the redis-sidekiq service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        Aggregation of all Redis operations issued to the Redis Sidekiq service from the Rails codebase.

        If this SLI is experiencing a degradation, it may be caused by saturation in the Redis Sidekiq instance caused by high traffic volumes from Sidekiq clients (Rails or other sidekiq jobs), or very large messages being delivered via Sidekiq.

        Reviewing Sidekiq job logs may help the investigation.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2248986914"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(gitlab_redis_client_requests_duration_seconds_bucket{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",storage="queues"}[5m])
          )
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="rails_redis_client",monitor="global",type="redis-sidekiq"}
          < (1 - 14.4 * 0.000100)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="rails_redis_client",monitor="global",type="redis-sidekiq"}
          < (1 - 14.4 * 0.000100)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="rails_redis_client",monitor="global",type="redis-sidekiq"}) >= 1
      )
  - alert: RedisSidekiqServiceRailsRedisClientApdexSLOViolation
    for: 2m
    annotations:
      title: The rails_redis_client SLI of the redis-sidekiq service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        Aggregation of all Redis operations issued to the Redis Sidekiq service from the Rails codebase.

        If this SLI is experiencing a degradation, it may be caused by saturation in the Redis Sidekiq instance caused by high traffic volumes from Sidekiq clients (Rails or other sidekiq jobs), or very large messages being delivered via Sidekiq.

        Reviewing Sidekiq job logs may help the investigation.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2248986914"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(gitlab_redis_client_requests_duration_seconds_bucket{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",storage="queues"}[5m])
          )
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="rails_redis_client",monitor="global",type="redis-sidekiq"}
          < (1 - 6 * 0.000100)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="rails_redis_client",monitor="global",type="redis-sidekiq"}
          < (1 - 6 * 0.000100)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="rails_redis_client",monitor="global",type="redis-sidekiq"}) >= 0.16667
      )
  - alert: RedisSidekiqServiceRailsRedisClientErrorSLOViolation
    for: 2m
    annotations:
      title: The rails_redis_client SLI of the redis-sidekiq service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Aggregation of all Redis operations issued to the Redis Sidekiq service from the Rails codebase.

        If this SLI is experiencing a degradation, it may be caused by saturation in the Redis Sidekiq instance caused by high traffic volumes from Sidekiq clients (Rails or other sidekiq jobs), or very large messages being delivered via Sidekiq.

        Reviewing Sidekiq job logs may help the investigation.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "487497488"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(gitlab_redis_client_exceptions_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",storage="queues"}[5m])
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="rails_redis_client",monitor="global",type="redis-sidekiq"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="rails_redis_client",monitor="global",type="redis-sidekiq"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="rails_redis_client",monitor="global",type="redis-sidekiq"}) >= 1
      )
  - alert: RedisSidekiqServiceRailsRedisClientErrorSLOViolation
    for: 2m
    annotations:
      title: The rails_redis_client SLI of the redis-sidekiq service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Aggregation of all Redis operations issued to the Redis Sidekiq service from the Rails codebase.

        If this SLI is experiencing a degradation, it may be caused by saturation in the Redis Sidekiq instance caused by high traffic volumes from Sidekiq clients (Rails or other sidekiq jobs), or very large messages being delivered via Sidekiq.

        Reviewing Sidekiq job logs may help the investigation.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "487497488"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(gitlab_redis_client_exceptions_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",storage="queues"}[5m])
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="rails_redis_client",monitor="global",type="redis-sidekiq"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="rails_redis_client",monitor="global",type="redis-sidekiq"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="rails_redis_client",monitor="global",type="redis-sidekiq"}) >= 0.16667
      )
  - alert: RedisSidekiqServiceRailsRedisClientTrafficCessation
    for: 5m
    annotations:
      title: The rails_redis_client SLI of the redis-sidekiq service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        Aggregation of all Redis operations issued to the Redis Sidekiq service from the Rails codebase.

        If this SLI is experiencing a degradation, it may be caused by saturation in the Redis Sidekiq instance caused by high traffic volumes from Sidekiq clients (Rails or other sidekiq jobs), or very large messages being delivered via Sidekiq.

        Reviewing Sidekiq job logs may help the investigation.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3147312091"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(gitlab_redis_client_requests_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",storage="queues"}[5m])
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_30m{component="rails_redis_client",monitor="global",stage="main",type="redis-sidekiq"} == 0
      and
      gitlab_component_ops:rate_30m{component="rails_redis_client",monitor="global",stage="main",type="redis-sidekiq"} offset 1h >= 0.16666666666666666
  - alert: RedisSidekiqServiceRailsRedisClientTrafficAbsent
    for: 30m
    annotations:
      title: The rails_redis_client SLI of the redis-sidekiq service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        Aggregation of all Redis operations issued to the Redis Sidekiq service from the Rails codebase.

        If this SLI is experiencing a degradation, it may be caused by saturation in the Redis Sidekiq instance caused by high traffic volumes from Sidekiq clients (Rails or other sidekiq jobs), or very large messages being delivered via Sidekiq.

        Reviewing Sidekiq job logs may help the investigation.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3147312091"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(gitlab_redis_client_requests_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",storage="queues"}[5m])
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_5m{component="rails_redis_client",monitor="global",stage="main",type="redis-sidekiq"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="rails_redis_client",monitor="global",stage="main",type="redis-sidekiq"}
  - alert: RedisSidekiqServiceSecondaryServersTrafficCessation
    for: 5m
    annotations:
      title: The secondary_servers SLI of the redis-sidekiq service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        Operations on the Redis secondaries for the Redis Sidekiq instance.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3020974736"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(redis_commands_processed_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="redis-sidekiq"}[5m]) and on (instance) redis_instance_info{role="slave"}
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_30m{component="secondary_servers",monitor="global",stage="main",type="redis-sidekiq"} == 0
      and
      gitlab_component_ops:rate_30m{component="secondary_servers",monitor="global",stage="main",type="redis-sidekiq"} offset 1h >= 0.16666666666666666
  - alert: RedisSidekiqServiceSecondaryServersTrafficAbsent
    for: 30m
    annotations:
      title: The secondary_servers SLI of the redis-sidekiq service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        Operations on the Redis secondaries for the Redis Sidekiq instance.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3020974736"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(redis_commands_processed_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="redis-sidekiq"}[5m]) and on (instance) redis_instance_info{role="slave"}
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_5m{component="secondary_servers",monitor="global",stage="main",type="redis-sidekiq"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="secondary_servers",monitor="global",stage="main",type="redis-sidekiq"}
